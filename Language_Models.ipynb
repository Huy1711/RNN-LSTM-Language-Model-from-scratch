{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 01 Word Embedding and Language Models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BintR5QEhbML"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PanzxhVXhbMD"
      },
      "source": [
        "## Introduction\n",
        "In this assignment, you will learn how to build a language model from scratch and use the model to generate new text.\n",
        "You will also see how training a language model helps you learn word representation.\n",
        "\n",
        "Note: \n",
        "- Plagiarism will result in 0 mark.\n",
        "- The following template shows how your code should look like. You are free to add more functions, change the parameters. You are not allowed to use existing implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBfeQLRQhbMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60a5139-2686-4e9e-e3f1-4c570a60cdc5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as ag\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math \n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tREmxGdthbMK"
      },
      "source": [
        "## Recurrent Neural Network (5 points)\n",
        "To begin, you have to implement the vanila RNN in Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOIuQLXaGkaA"
      },
      "source": [
        "### Vanila RNN\n",
        "Recall the formula for vanila RNN:\n",
        "        \\begin{eqnarray}\n",
        "        h_t & = & \\sigma(W_h h_{t-1} + W_x x_t + b_1) \\\\\n",
        "        y_t & = & \\phi(W_y h_t + b_2)\n",
        "        \\end{eqnarray}\n",
        "where $\\sigma$ is the usually the sigmoid activation function and $\\phi$ is usually the softmax function.\n",
        "\n",
        "Hints:\n",
        "For RNNLM, the input is a squence of word_id, e.g. [10, 8, 5, 2, 101, 23]. You have to convert each word_id to an embedding vector. To implement this, you can use the `torch.nn.Embedding` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS19GEnShbMK"
      },
      "source": [
        "class VanilaRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a vanila RNN. \n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons = embedding_dim\n",
        "        n_hiddens: number of hidden neurons \n",
        "        n_outputs: number of output neurons = vocab_size\n",
        "        vocab: a dictionary of the form {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super(VanilaRNNLM, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(n_outputs, n_inputs)\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.W_e = nn.Parameter(torch.Tensor(n_inputs, n_hiddens).to(device))\n",
        "        self.W_h = nn.Parameter(torch.Tensor(n_hiddens, n_hiddens).to(device))\n",
        "        self.b1 = nn.Parameter(torch.zeros(1, n_hiddens).to(device))\n",
        "        \n",
        "        self.W_y = nn.Parameter(torch.Tensor(n_hiddens, n_outputs).to(device))\n",
        "        self.b2 = nn.Parameter(torch.zeros(1, n_outputs))\n",
        "\n",
        "        # initialize weights\n",
        "        nn.init.uniform_(self.embeddings.weight, -1.0, 1.0)\n",
        "        nn.init.xavier_normal_(self.W_e)\n",
        "        nn.init.xavier_normal_(self.W_h)\n",
        "        nn.init.xavier_normal_(self.W_y)\n",
        "\n",
        "    \n",
        "    def forward(self, xs, h0):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "        xs: the input sequence [x_1, x_2, ..., x_n]. x_i is the id of the i-th word in the sequence. \n",
        "            For example, xs = [1, 3, 11, 6, 8, 2]\n",
        "        h0: the initial hidden state\n",
        "        \n",
        "        Returns: (ys, hs) where\n",
        "        ys = [y_1, y_2, ..., y_n] and\n",
        "        hs = [h_1, h_2, ..., h_n]\n",
        "        \"\"\"\n",
        "        # embedded sequence [e_1, e_2, ..., e_n]    n x embedding_dim\n",
        "        es = self.embeddings(xs)  \n",
        "        #.view((1, -1))\n",
        "\n",
        "        Time_steps = len(es)\n",
        "        hs = []\n",
        "        ys = []\n",
        "\n",
        "        for t in range(Time_steps):\n",
        "          if (t==0):\n",
        "            # h_1 = sigmoid(W_h * h0 + W_e * e_1 + b1)  \n",
        "            h_t = torch.matmul(h0, self.W_h) + torch.matmul(es[t], self.W_e)  + self.b1\n",
        "            h_t = torch.sigmoid(h_t)\n",
        "            hs.append(h_t)\n",
        "          else:\n",
        "            # h_t = sigmoid(W_h * h_(t-1) + W_e * e_t + b1)  \n",
        "            h_t = torch.matmul(hs[t-1], self.W_h) + torch.matmul(es[t], self.W_e)  + self.b1\n",
        "            h_t = torch.sigmoid(h_t)\n",
        "            hs.append(h_t)\n",
        "          # y_t = softmax(W_y * h_t + b2)\n",
        "          y_t = torch.matmul(hs[t], self.W_y) + self.b2\n",
        "          ys.append(y_t)\n",
        "\n",
        "        return ys, hs\n",
        "        "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BintR5QEhbML"
      },
      "source": [
        "### Fancy RNN\n",
        "Vanila RNN suffers from the gradient vanishing/exploding problem. Your next task is to implement a more sophisticated RNN that is more robust to gradient vanishing/exploding.\n",
        "\n",
        "By reducing the vanishing gradient problem, the LSTM architecture makes it easier for the RNN to preserve information from many previous timesteps. So that the LSTM can learn long-distance dependencies.\n",
        "\n",
        "$$ f^{(t)} = \\sigma(W_f h^{(t-1)} + U_f x^{(t)} + b_f) $$\n",
        "$$ i^{(t)} = \\sigma(W_i h^{(t-1)} + U_i x^{(t)} + b_i) $$\n",
        "$$ o^{(t)} = \\sigma(W_o h^{(t-1)} + U_o x^{(t)} + b_o) $$\n",
        "\n",
        "$$ g^{(t)} = \\tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c) $$\n",
        "$$ c^{(t)} = f^{(t)} \\odot c^{(t-1)} + i^{(t)} \\odot \\tilde{c}^{(t)} $$\n",
        "$$ h^{(t)} = o^{(t)} \\odot \\tanh c^{(t)} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIZL3TLXhbML"
      },
      "source": [
        "class FancyRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, n_layers, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a fancy RNN, this could be LSTM, GRU, or your own invention.\n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons\n",
        "        n_hiddens: number of hidden neurons\n",
        "        n_outputs: number of output neurons\n",
        "        vocab: a dictionary {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super(FancyRNNLM, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(n_outputs, n_inputs)\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.Tensor(n_hiddens, n_hiddens).to(device))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(n_inputs, n_hiddens).to(device))\n",
        "        self.b_f = nn.Parameter(torch.zeros(1, n_hiddens).to(device))\n",
        "        \n",
        "        self.W_i = nn.Parameter(torch.Tensor(n_hiddens, n_hiddens).to(device))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(n_inputs, n_hiddens).to(device))\n",
        "        self.b_i = nn.Parameter(torch.zeros(1, n_hiddens).to(device))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.Tensor(n_hiddens, n_hiddens).to(device))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(n_inputs, n_hiddens).to(device))\n",
        "        self.b_o = nn.Parameter(torch.zeros(1, n_hiddens).to(device))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.Tensor(n_hiddens, n_hiddens).to(device))\n",
        "        self.U_g = nn.Parameter(torch.Tensor(n_inputs, n_hiddens).to(device))\n",
        "        self.b_g = nn.Parameter(torch.zeros(1, n_hiddens).to(device))\n",
        "\n",
        "        self.fc = nn.Linear(n_hiddens, n_outputs)\n",
        "\n",
        "        # initialize weights\n",
        "        nn.init.uniform_(self.embeddings.weight, -1.0, 1.0)\n",
        "        nn.init.xavier_normal_(self.W_f)\n",
        "        nn.init.xavier_normal_(self.U_f)\n",
        "        nn.init.xavier_normal_(self.W_i)\n",
        "        nn.init.xavier_normal_(self.U_i)\n",
        "        nn.init.xavier_normal_(self.W_o)\n",
        "        nn.init.xavier_normal_(self.U_o)\n",
        "        nn.init.xavier_normal_(self.W_g)\n",
        "        nn.init.xavier_normal_(self.U_g)\n",
        "\n",
        "    def forward(self, xs, h0):\n",
        "\n",
        "        es = self.embeddings(xs)  \n",
        "\n",
        "        Time_steps = len(es)\n",
        "        cs = []\n",
        "        hs = []\n",
        "        ys = []\n",
        "        c0 = torch.zeros(1, self.n_hiddens).to(device)\n",
        "\n",
        "        for t in range(Time_steps):\n",
        "          if (t==0):\n",
        "            f_t = torch.sigmoid(torch.matmul(h0, self.W_f) + torch.matmul(es[t], self.U_f)  + self.b_f)\n",
        "            i_t = torch.sigmoid(torch.matmul(h0, self.W_i) + torch.matmul(es[t], self.U_i)  + self.b_i)\n",
        "            o_t = torch.sigmoid(torch.matmul(h0, self.W_o) + torch.matmul(es[t], self.U_o)  + self.b_o)\n",
        "            # g_t = tanh(W_g * h_(t-1) + U_g * e_t + b_g) \n",
        "            g_t = torch.tanh(torch.matmul(h0, self.W_g) + torch.matmul(es[t], self.U_g)  + self.b_g)\n",
        "            c_t = f_t * c0 + i_t * g_t\n",
        "            cs.append(c_t)\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hs.append(h_t)\n",
        "          else:\n",
        "            # f_t = sigmoid(W_f * h_(t-1) + W_f * e_t + b_f)  \n",
        "            f_t = torch.sigmoid(torch.matmul(hs[t-1], self.W_f) + torch.matmul(es[t], self.U_f)  + self.b_f)\n",
        "            i_t = torch.sigmoid(torch.matmul(hs[t-1], self.W_i) + torch.matmul(es[t], self.U_i)  + self.b_i)\n",
        "            o_t = torch.sigmoid(torch.matmul(hs[t-1], self.W_o) + torch.matmul(es[t], self.U_o)  + self.b_o)\n",
        "            # g_t = tanh(W_g * h_(t-1) + U_g * e_t + b_g) \n",
        "            g_t = torch.tanh(torch.matmul(hs[t-1], self.W_g) + torch.matmul(es[t], self.U_g)  + self.b_g)\n",
        "            c_t = f_t * cs[t-1] + i_t * g_t\n",
        "            cs.append(c_t)\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hs.append(h_t)\n",
        "          y_t = self.fc(h_t)\n",
        "          ys.append(y_t)\n",
        "\n",
        "        return ys, hs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdZzai2vhbMN"
      },
      "source": [
        "## Language Modeling with RNN (4 points)\n",
        "The next step is to use our RNNs in some real world tasks. One of the most common application of RNN is language modeling.\n",
        "\n",
        "### Data\n",
        "For this assignment, we will use text data from Wikipedia. To start, download the data from this website:\n",
        "\n",
        "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
        "\n",
        "Some information about this dataset can be found here:\n",
        "\n",
        "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKd3ycYoUndO"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# download the data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip -P \"/content/\"\n",
        "\n",
        "with ZipFile('/content/wikitext-103-v1.zip', 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    zip.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wicr2If_hOSk"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "train_data = Path('/content/wikitext-103/wiki.test.tokens').read_text()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6wseHx2se59"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSzE17_GI3xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d96c4fe-7c2f-4e0a-f304-07d88738553f"
      },
      "source": [
        "# Divide heading and article of training set\n",
        "heading_pattern = '( \\n \\n = [^=]*[^=] = \\n \\n )'\n",
        "breakline = ' \\n'\n",
        "train_data = breakline + train_data\n",
        "train_split = re.split(heading_pattern, train_data)\n",
        "train_headings = [x for x in train_split[1::2]]\n",
        "train_articles = [x for x in train_split[2::2]]\n",
        "\n",
        "print(train_headings[0])\n",
        "print(train_articles[0][:100])\n",
        "print(len(train_articles))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            " \n",
            " = Robert Boulter = \n",
            " \n",
            " \n",
            "Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role \n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP_PdNILHFRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8726ea0a-5d04-42fd-9f3e-438c24c4a29e"
      },
      "source": [
        "# tokenization + lowercasing\n",
        "train_articles_token = [article.lower().split() for article in train_articles]\n",
        "\n",
        "print(train_articles_token[0][:10]) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['robert', 'boulter', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asVJwAdaywel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4db6da-17b0-41d1-f3f7-1dcb71c31241"
      },
      "source": [
        "# create vocabulary\n",
        "vocabulary = []\n",
        "for sentence in train_articles_token:\n",
        "  for token in sentence:\n",
        "    if token not in vocabulary:\n",
        "      vocabulary.append(token)\n",
        "\n",
        "# word2id and id2word dictionaries\n",
        "word2id = {w: id for (id, w) in enumerate(vocabulary)}\n",
        "id2word = {id: w for (id, w) in enumerate(vocabulary)}\n",
        "\n",
        "for iter in range(10):\n",
        "  print(str(iter) + ':' + id2word[iter])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:robert\n",
            "1:boulter\n",
            "2:is\n",
            "3:an\n",
            "4:english\n",
            "5:film\n",
            "6:,\n",
            "7:television\n",
            "8:and\n",
            "9:theatre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGZZoPxx_0QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6540138-58b0-4b8f-e513-6a77ff70df72"
      },
      "source": [
        "# create corpus\n",
        "corpus = []\n",
        "\n",
        "for article in train_articles:\n",
        "  sentences = article.lower().split(\" . \")\n",
        "  for sentence in sentences:\n",
        "    if (len(sentence) > 1):\n",
        "      corpus.append(sentence)\n",
        "\n",
        "corpus_to_id = []\n",
        "\n",
        "for sentence in corpus:\n",
        "  tokens_id = []\n",
        "  sentence_to_tokens = sentence.split()\n",
        "  for token in sentence_to_tokens:\n",
        "    token_id = word2id[token]\n",
        "    tokens_id.append(token_id)\n",
        "  if (len(tokens_id) > 1):\n",
        "    corpus_to_id.append(tokens_id)\n",
        "\n",
        "print(corpus[0])\n",
        "print(corpus_to_id[0])\n",
        "print(len(corpus_to_id))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "robert boulter is an english film , television and theatre actor\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "8927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvnzMZkd-FFL"
      },
      "source": [
        "corpus2 = corpus_to_id[:500]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLQnfaaqhbMO"
      },
      "source": [
        "### Training a LM with RNN\n",
        "Write the code to train RNNLMs with the VanilaRNNLM and FancyRNNLM classes above. Train 1 instance of VanilaRNNLM and 1 instance of FancyRNNLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFCzSIhVrbul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b008bf-b45b-45cc-9cf6-cec747c30b7f"
      },
      "source": [
        "vocab_size = len(word2id)\n",
        "embedding_dim = 12    # 24\n",
        "\n",
        "input_size = embedding_dim\n",
        "hidden_size = 128\n",
        "output_size = vocab_size\n",
        "\n",
        "print(vocab_size)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxbDIphOq-Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b94a6b-7998-4e1b-cb9c-064d7475fa5e"
      },
      "source": [
        "vanila_rnn = VanilaRNNLM(n_inputs=input_size, n_hiddens=hidden_size, n_outputs=output_size, vocab=word2id)\n",
        "vanila_rnn = vanila_rnn.to(device)\n",
        "for name, param in vanila_rnn.named_parameters():  \n",
        "  print(name, param.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_e torch.Size([12, 128])\n",
            "W_h torch.Size([128, 128])\n",
            "b1 torch.Size([1, 128])\n",
            "W_y torch.Size([128, 17555])\n",
            "b2 torch.Size([1, 17555])\n",
            "embeddings.weight torch.Size([17555, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGTWYSSNEgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e5d74c-b5f9-4ed9-9436-3ce1d2d2bb9b"
      },
      "source": [
        "fancy_rnn = FancyRNNLM(n_inputs=input_size, n_hiddens=hidden_size, n_outputs=output_size, n_layers=1, vocab=word2id)\n",
        "fancy_rnn = fancy_rnn.to(device)\n",
        "for name, param in fancy_rnn.named_parameters():  \n",
        "  print(name, param.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_f torch.Size([128, 128])\n",
            "U_f torch.Size([12, 128])\n",
            "b_f torch.Size([1, 128])\n",
            "W_i torch.Size([128, 128])\n",
            "U_i torch.Size([12, 128])\n",
            "b_i torch.Size([1, 128])\n",
            "W_o torch.Size([128, 128])\n",
            "U_o torch.Size([12, 128])\n",
            "b_o torch.Size([1, 128])\n",
            "W_g torch.Size([128, 128])\n",
            "U_g torch.Size([12, 128])\n",
            "b_g torch.Size([1, 128])\n",
            "embeddings.weight torch.Size([17555, 12])\n",
            "fc.weight torch.Size([17555, 128])\n",
            "fc.bias torch.Size([17555])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVd6aT0_hbMP"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_rnnlm(corpus, rnnlm, **train_params):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    corpus: the text corpus\n",
        "    rnnlm: the RNN\n",
        "    train_params: other parameters, e.g. learning rate, batch size, number of GPUs, ...\n",
        "    \"\"\"\n",
        "    num_epochs = 25\n",
        "    print_everywhere = 100\n",
        "    count = 0\n",
        "    \n",
        "    lr = 0.002\n",
        "\n",
        "    # Create optimizers\n",
        "    optimizer = optim.Adam(rnnlm.parameters(), lr)\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model_loss = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "      for sentence in corpus:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # init h0\n",
        "        h0 = torch.zeros(1, hidden_size).to(device)\n",
        "\n",
        "        # input sentence\n",
        "        input_sentence = torch.LongTensor(sentence[:-1]).to(device)\n",
        "        \n",
        "        # target sentence\n",
        "        target_sentence = torch.LongTensor(sentence[1:]).to(device)\n",
        "        \n",
        "        # forward \n",
        "        ys, hs = rnnlm(input_sentence, h0)\n",
        "        ys = torch.cat(ys)\n",
        "        loss = criterion(ys, target_sentence)\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent update\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if (count % print_everywhere == 0):\n",
        "          print('Epoch [{:5d}/{:5d}] | loss: {:6.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "          count = 0\n",
        "\n",
        "      model_loss.append(loss.item())\n",
        "\n",
        "    return rnnlm, model_loss"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJsmrfo3hbMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4cfeff-6ce7-4c3b-e8c5-14645aea90dc"
      },
      "source": [
        "vanila_rnnlm, vanila_rnnlm_loss = train_rnnlm(corpus=corpus2, rnnlm=vanila_rnn)\n",
        "vanila_rnnlm = vanila_rnnlm.to(device)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [    1/   25] | loss: 7.6329\n",
            "Epoch [    1/   25] | loss: 8.1594\n",
            "Epoch [    1/   25] | loss: 8.0024\n",
            "Epoch [    1/   25] | loss: 7.3320\n",
            "Epoch [    1/   25] | loss: 6.6917\n",
            "Epoch [    2/   25] | loss: 6.4327\n",
            "Epoch [    2/   25] | loss: 6.9208\n",
            "Epoch [    2/   25] | loss: 6.4066\n",
            "Epoch [    2/   25] | loss: 6.2132\n",
            "Epoch [    2/   25] | loss: 6.1785\n",
            "Epoch [    3/   25] | loss: 6.2284\n",
            "Epoch [    3/   25] | loss: 6.5570\n",
            "Epoch [    3/   25] | loss: 6.0670\n",
            "Epoch [    3/   25] | loss: 5.7876\n",
            "Epoch [    3/   25] | loss: 5.7901\n",
            "Epoch [    4/   25] | loss: 6.0717\n",
            "Epoch [    4/   25] | loss: 6.2539\n",
            "Epoch [    4/   25] | loss: 5.8142\n",
            "Epoch [    4/   25] | loss: 5.4708\n",
            "Epoch [    4/   25] | loss: 5.4371\n",
            "Epoch [    5/   25] | loss: 5.8687\n",
            "Epoch [    5/   25] | loss: 5.9561\n",
            "Epoch [    5/   25] | loss: 5.5287\n",
            "Epoch [    5/   25] | loss: 5.1750\n",
            "Epoch [    5/   25] | loss: 5.0907\n",
            "Epoch [    6/   25] | loss: 5.5089\n",
            "Epoch [    6/   25] | loss: 5.6260\n",
            "Epoch [    6/   25] | loss: 5.2501\n",
            "Epoch [    6/   25] | loss: 4.8507\n",
            "Epoch [    6/   25] | loss: 4.7596\n",
            "Epoch [    7/   25] | loss: 5.3203\n",
            "Epoch [    7/   25] | loss: 5.2414\n",
            "Epoch [    7/   25] | loss: 4.9242\n",
            "Epoch [    7/   25] | loss: 4.5155\n",
            "Epoch [    7/   25] | loss: 4.4654\n",
            "Epoch [    8/   25] | loss: 4.9653\n",
            "Epoch [    8/   25] | loss: 4.7356\n",
            "Epoch [    8/   25] | loss: 4.6191\n",
            "Epoch [    8/   25] | loss: 4.1101\n",
            "Epoch [    8/   25] | loss: 4.1325\n",
            "Epoch [    9/   25] | loss: 4.4632\n",
            "Epoch [    9/   25] | loss: 4.2990\n",
            "Epoch [    9/   25] | loss: 4.3482\n",
            "Epoch [    9/   25] | loss: 3.7167\n",
            "Epoch [    9/   25] | loss: 3.8187\n",
            "Epoch [   10/   25] | loss: 4.1232\n",
            "Epoch [   10/   25] | loss: 3.9360\n",
            "Epoch [   10/   25] | loss: 4.0605\n",
            "Epoch [   10/   25] | loss: 3.2450\n",
            "Epoch [   10/   25] | loss: 3.5000\n",
            "Epoch [   11/   25] | loss: 3.7851\n",
            "Epoch [   11/   25] | loss: 3.4949\n",
            "Epoch [   11/   25] | loss: 3.7383\n",
            "Epoch [   11/   25] | loss: 2.8915\n",
            "Epoch [   11/   25] | loss: 3.2255\n",
            "Epoch [   12/   25] | loss: 3.5187\n",
            "Epoch [   12/   25] | loss: 3.0917\n",
            "Epoch [   12/   25] | loss: 3.4587\n",
            "Epoch [   12/   25] | loss: 2.6343\n",
            "Epoch [   12/   25] | loss: 2.9531\n",
            "Epoch [   13/   25] | loss: 3.1729\n",
            "Epoch [   13/   25] | loss: 2.7276\n",
            "Epoch [   13/   25] | loss: 3.1904\n",
            "Epoch [   13/   25] | loss: 2.3607\n",
            "Epoch [   13/   25] | loss: 2.7181\n",
            "Epoch [   14/   25] | loss: 2.9361\n",
            "Epoch [   14/   25] | loss: 2.4999\n",
            "Epoch [   14/   25] | loss: 2.9886\n",
            "Epoch [   14/   25] | loss: 2.1010\n",
            "Epoch [   14/   25] | loss: 2.5898\n",
            "Epoch [   15/   25] | loss: 2.9239\n",
            "Epoch [   15/   25] | loss: 2.3058\n",
            "Epoch [   15/   25] | loss: 2.9284\n",
            "Epoch [   15/   25] | loss: 1.9493\n",
            "Epoch [   15/   25] | loss: 2.4540\n",
            "Epoch [   16/   25] | loss: 2.8239\n",
            "Epoch [   16/   25] | loss: 2.1736\n",
            "Epoch [   16/   25] | loss: 2.7627\n",
            "Epoch [   16/   25] | loss: 1.7941\n",
            "Epoch [   16/   25] | loss: 2.3407\n",
            "Epoch [   17/   25] | loss: 2.7715\n",
            "Epoch [   17/   25] | loss: 1.9682\n",
            "Epoch [   17/   25] | loss: 2.6803\n",
            "Epoch [   17/   25] | loss: 1.7802\n",
            "Epoch [   17/   25] | loss: 2.2579\n",
            "Epoch [   18/   25] | loss: 2.6895\n",
            "Epoch [   18/   25] | loss: 1.8258\n",
            "Epoch [   18/   25] | loss: 2.4951\n",
            "Epoch [   18/   25] | loss: 1.5638\n",
            "Epoch [   18/   25] | loss: 2.1282\n",
            "Epoch [   19/   25] | loss: 2.6465\n",
            "Epoch [   19/   25] | loss: 1.5933\n",
            "Epoch [   19/   25] | loss: 2.3634\n",
            "Epoch [   19/   25] | loss: 1.4348\n",
            "Epoch [   19/   25] | loss: 1.9842\n",
            "Epoch [   20/   25] | loss: 2.4834\n",
            "Epoch [   20/   25] | loss: 1.5556\n",
            "Epoch [   20/   25] | loss: 2.3525\n",
            "Epoch [   20/   25] | loss: 1.4516\n",
            "Epoch [   20/   25] | loss: 1.8679\n",
            "Epoch [   21/   25] | loss: 2.4253\n",
            "Epoch [   21/   25] | loss: 1.4090\n",
            "Epoch [   21/   25] | loss: 2.2044\n",
            "Epoch [   21/   25] | loss: 1.3446\n",
            "Epoch [   21/   25] | loss: 1.7787\n",
            "Epoch [   22/   25] | loss: 2.3206\n",
            "Epoch [   22/   25] | loss: 1.6734\n",
            "Epoch [   22/   25] | loss: 2.1016\n",
            "Epoch [   22/   25] | loss: 1.1549\n",
            "Epoch [   22/   25] | loss: 1.7731\n",
            "Epoch [   23/   25] | loss: 2.2143\n",
            "Epoch [   23/   25] | loss: 1.4600\n",
            "Epoch [   23/   25] | loss: 2.1103\n",
            "Epoch [   23/   25] | loss: 1.0638\n",
            "Epoch [   23/   25] | loss: 1.5781\n",
            "Epoch [   24/   25] | loss: 2.0891\n",
            "Epoch [   24/   25] | loss: 1.3517\n",
            "Epoch [   24/   25] | loss: 2.6163\n",
            "Epoch [   24/   25] | loss: 1.2930\n",
            "Epoch [   24/   25] | loss: 1.6472\n",
            "Epoch [   25/   25] | loss: 2.1346\n",
            "Epoch [   25/   25] | loss: 1.1847\n",
            "Epoch [   25/   25] | loss: 2.9671\n",
            "Epoch [   25/   25] | loss: 1.0282\n",
            "Epoch [   25/   25] | loss: 1.5074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbznpvGqONAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295fd72b-a221-41d4-fa66-6233e93a3727"
      },
      "source": [
        "fancy_rnnlm, fancy_rnnlm_loss = train_rnnlm(corpus=corpus2, rnnlm=fancy_rnn)\n",
        "fancy_rnnlm = fancy_rnnlm.to(device)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [    1/   25] | loss: 7.7299\n",
            "Epoch [    1/   25] | loss: 7.9052\n",
            "Epoch [    1/   25] | loss: 7.5175\n",
            "Epoch [    1/   25] | loss: 6.9154\n",
            "Epoch [    1/   25] | loss: 6.5246\n",
            "Epoch [    2/   25] | loss: 6.3884\n",
            "Epoch [    2/   25] | loss: 6.4987\n",
            "Epoch [    2/   25] | loss: 5.8850\n",
            "Epoch [    2/   25] | loss: 5.7136\n",
            "Epoch [    2/   25] | loss: 5.8173\n",
            "Epoch [    3/   25] | loss: 6.0454\n",
            "Epoch [    3/   25] | loss: 6.2241\n",
            "Epoch [    3/   25] | loss: 5.4125\n",
            "Epoch [    3/   25] | loss: 5.2079\n",
            "Epoch [    3/   25] | loss: 5.4240\n",
            "Epoch [    4/   25] | loss: 5.6987\n",
            "Epoch [    4/   25] | loss: 5.4587\n",
            "Epoch [    4/   25] | loss: 4.9877\n",
            "Epoch [    4/   25] | loss: 4.7803\n",
            "Epoch [    4/   25] | loss: 4.8924\n",
            "Epoch [    5/   25] | loss: 5.2383\n",
            "Epoch [    5/   25] | loss: 5.0818\n",
            "Epoch [    5/   25] | loss: 5.0021\n",
            "Epoch [    5/   25] | loss: 4.3849\n",
            "Epoch [    5/   25] | loss: 4.4251\n",
            "Epoch [    6/   25] | loss: 4.9488\n",
            "Epoch [    6/   25] | loss: 4.5032\n",
            "Epoch [    6/   25] | loss: 4.5389\n",
            "Epoch [    6/   25] | loss: 4.0811\n",
            "Epoch [    6/   25] | loss: 3.9592\n",
            "Epoch [    7/   25] | loss: 4.9974\n",
            "Epoch [    7/   25] | loss: 4.1353\n",
            "Epoch [    7/   25] | loss: 4.4331\n",
            "Epoch [    7/   25] | loss: 3.4889\n",
            "Epoch [    7/   25] | loss: 3.4970\n",
            "Epoch [    8/   25] | loss: 4.5915\n",
            "Epoch [    8/   25] | loss: 3.6331\n",
            "Epoch [    8/   25] | loss: 4.1328\n",
            "Epoch [    8/   25] | loss: 3.2813\n",
            "Epoch [    8/   25] | loss: 3.2458\n",
            "Epoch [    9/   25] | loss: 4.5209\n",
            "Epoch [    9/   25] | loss: 3.1091\n",
            "Epoch [    9/   25] | loss: 3.8226\n",
            "Epoch [    9/   25] | loss: 3.0466\n",
            "Epoch [    9/   25] | loss: 3.0838\n",
            "Epoch [   10/   25] | loss: 4.3292\n",
            "Epoch [   10/   25] | loss: 2.6393\n",
            "Epoch [   10/   25] | loss: 3.3218\n",
            "Epoch [   10/   25] | loss: 2.8931\n",
            "Epoch [   10/   25] | loss: 2.7021\n",
            "Epoch [   11/   25] | loss: 3.8640\n",
            "Epoch [   11/   25] | loss: 2.7560\n",
            "Epoch [   11/   25] | loss: 3.1980\n",
            "Epoch [   11/   25] | loss: 2.5250\n",
            "Epoch [   11/   25] | loss: 2.6512\n",
            "Epoch [   12/   25] | loss: 3.4437\n",
            "Epoch [   12/   25] | loss: 2.2998\n",
            "Epoch [   12/   25] | loss: 3.0218\n",
            "Epoch [   12/   25] | loss: 2.2952\n",
            "Epoch [   12/   25] | loss: 2.4796\n",
            "Epoch [   13/   25] | loss: 3.3640\n",
            "Epoch [   13/   25] | loss: 2.0213\n",
            "Epoch [   13/   25] | loss: 3.1226\n",
            "Epoch [   13/   25] | loss: 1.9122\n",
            "Epoch [   13/   25] | loss: 2.0661\n",
            "Epoch [   14/   25] | loss: 3.3415\n",
            "Epoch [   14/   25] | loss: 1.7698\n",
            "Epoch [   14/   25] | loss: 2.7669\n",
            "Epoch [   14/   25] | loss: 1.7441\n",
            "Epoch [   14/   25] | loss: 1.9819\n",
            "Epoch [   15/   25] | loss: 3.3383\n",
            "Epoch [   15/   25] | loss: 1.4289\n",
            "Epoch [   15/   25] | loss: 2.6536\n",
            "Epoch [   15/   25] | loss: 1.6469\n",
            "Epoch [   15/   25] | loss: 1.7383\n",
            "Epoch [   16/   25] | loss: 3.4042\n",
            "Epoch [   16/   25] | loss: 1.3193\n",
            "Epoch [   16/   25] | loss: 2.5286\n",
            "Epoch [   16/   25] | loss: 1.5661\n",
            "Epoch [   16/   25] | loss: 1.6084\n",
            "Epoch [   17/   25] | loss: 3.0075\n",
            "Epoch [   17/   25] | loss: 1.5417\n",
            "Epoch [   17/   25] | loss: 2.6714\n",
            "Epoch [   17/   25] | loss: 1.2984\n",
            "Epoch [   17/   25] | loss: 1.2865\n",
            "Epoch [   18/   25] | loss: 3.1427\n",
            "Epoch [   18/   25] | loss: 1.2869\n",
            "Epoch [   18/   25] | loss: 2.3726\n",
            "Epoch [   18/   25] | loss: 1.2966\n",
            "Epoch [   18/   25] | loss: 1.3244\n",
            "Epoch [   19/   25] | loss: 2.6739\n",
            "Epoch [   19/   25] | loss: 1.2179\n",
            "Epoch [   19/   25] | loss: 1.9745\n",
            "Epoch [   19/   25] | loss: 1.0475\n",
            "Epoch [   19/   25] | loss: 1.0497\n",
            "Epoch [   20/   25] | loss: 2.7029\n",
            "Epoch [   20/   25] | loss: 1.4763\n",
            "Epoch [   20/   25] | loss: 2.0598\n",
            "Epoch [   20/   25] | loss: 0.8823\n",
            "Epoch [   20/   25] | loss: 1.1180\n",
            "Epoch [   21/   25] | loss: 2.2824\n",
            "Epoch [   21/   25] | loss: 1.0973\n",
            "Epoch [   21/   25] | loss: 1.8239\n",
            "Epoch [   21/   25] | loss: 0.6523\n",
            "Epoch [   21/   25] | loss: 0.8694\n",
            "Epoch [   22/   25] | loss: 2.1309\n",
            "Epoch [   22/   25] | loss: 0.9592\n",
            "Epoch [   22/   25] | loss: 1.5586\n",
            "Epoch [   22/   25] | loss: 0.7556\n",
            "Epoch [   22/   25] | loss: 0.6711\n",
            "Epoch [   23/   25] | loss: 1.9852\n",
            "Epoch [   23/   25] | loss: 0.9745\n",
            "Epoch [   23/   25] | loss: 1.5570\n",
            "Epoch [   23/   25] | loss: 0.6064\n",
            "Epoch [   23/   25] | loss: 0.8443\n",
            "Epoch [   24/   25] | loss: 1.9590\n",
            "Epoch [   24/   25] | loss: 0.7037\n",
            "Epoch [   24/   25] | loss: 1.7988\n",
            "Epoch [   24/   25] | loss: 0.4915\n",
            "Epoch [   24/   25] | loss: 0.6281\n",
            "Epoch [   25/   25] | loss: 1.8844\n",
            "Epoch [   25/   25] | loss: 0.6916\n",
            "Epoch [   25/   25] | loss: 1.4997\n",
            "Epoch [   25/   25] | loss: 0.5367\n",
            "Epoch [   25/   25] | loss: 0.6773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBIXr8G-QX5K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "6e4fbe6d-4efb-422f-d0f1-28e602576374"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "vanila_rnnlm_loss = np.array(vanila_rnnlm_loss)\n",
        "fancy_rnnlm_loss = np.array(fancy_rnnlm_loss)\n",
        "plt.plot(vanila_rnnlm_loss, label='RNN')\n",
        "plt.plot(fancy_rnnlm_loss, label='FancyRNN')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8ab2a2ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkStLG4MhbMQ"
      },
      "source": [
        "### Generating new text with RNNLM\n",
        "Write the code to generate new text segments from the RNNLM. Produce several outputs from both VanilaRNN and FancyRNN to compare the quality of 2 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-7yqNaohbMQ"
      },
      "source": [
        "def generate_text(rnnlm, seed_text, length, **params):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    rnnlm: the language model\n",
        "    seed_text: a string of initial text\n",
        "    length: the length of the generated text\n",
        "    params: other params\n",
        "    \"\"\"\n",
        "    seed_text = seed_text.lower().split() # ['students', 'open', 'their', 'bag']\n",
        "\n",
        "    # translate words to id\n",
        "    sentence_id = []\n",
        "    for word in seed_text:\n",
        "      sentence_id.append(word2id[word])    # ['123', '435', '135', '14']\n",
        "    input_tensor = torch.LongTensor(sentence_id).to(device)\n",
        "\n",
        "    # put seed_text through the language model\n",
        "    for i in range(length):\n",
        "      h0 = torch.zeros(1, hidden_size).to(device)\n",
        "      output_text, hs = rnnlm(input_tensor, h0)\n",
        "      output_text = output_text[-1]\n",
        "      output_text = torch.squeeze(output_text)\n",
        "\n",
        "      # take the word with the highest probability\n",
        "      argmax_id = torch.argmax(output_text).item()\n",
        "      sentence_id.append(argmax_id)            # ['123', '435', '135', '14', '78']\n",
        "      input_tensor = torch.LongTensor(sentence_id).to(device)\n",
        "\n",
        "    # translate id to words\n",
        "    output_sentence = []\n",
        "    for id in sentence_id:\n",
        "      output_sentence.append(id2word[id])\n",
        "    output_sentence = ' '.join(output_sentence)\n",
        "\n",
        "    return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SbpTF0hbMR"
      },
      "source": [
        "seed_text = input('Enter your initial text: ')\n",
        "# output_text = generate_text(rnnlm=vanila_rnnlm, seed_text=seed_text, length=100)\n",
        "output_text = generate_text(rnnlm=fancy_rnnlm, seed_text=seed_text, length=100)\n",
        "print('{:<25}{:<30}'.format('Output sentence: ', output_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-PRkw_TkSUH"
      },
      "source": [
        "# Test with sentences in the corpus\n",
        "corpus_id = 52       # [0, corpus_size]\n",
        "seed_length = 10     # [1, sentence_length]\n",
        "output_length = 20\n",
        "seed_text = corpus[corpus_id].split()\n",
        "seed_text = seed_text[:seed_length]\n",
        "seed_text = ' '.join(seed_text)\n",
        "\n",
        "output_text = generate_text(rnnlm=vanila_rnnlm, seed_text=seed_text, length=output_length)\n",
        "output_text = generate_text(rnnlm=fancy_rnnlm, seed_text=seed_text, length=output_length)\n",
        "\n",
        "print('{:<20}{:<30}'.format('True sentence: ', corpus[corpus_id]))\n",
        "print('{:<20}{:<30}'.format('Output sentence: ', output_text))\n",
        "print('{:<20}{:<30}'.format('Output sentence: ', output_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbyfxhT9hbMR"
      },
      "source": [
        "### Perplexity (+2 bonus points)\n",
        "Compute the perplexity of the models. The lower the perplexity, the higher your score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfIUjATxhbMR"
      },
      "source": [
        "def perplexity(rnnlm, corpus):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeMBIuM1hbMR"
      },
      "source": [
        "# perp = perplexity(rnnlm=rnnlm, corpus=wiki_corpus_test)\n",
        "# print(perp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUnAS3GChbMS"
      },
      "source": [
        "## Word Embedding (1 point + 1 bonus point)\n",
        "\n",
        "Now you have trained your RNNLM, the `torch.nn.Embedding` layer in your model stores the embeddings of words in the dictionary. You can use dimensionality reduction algorithms such as PCA and TSNE to visualize the word embeddings.\n",
        "Produce a 2D plot of 100 to 1000 words and write a short analysis of the plot (e.g. the clusters of words with similar meaning, arithmetic operations you can apply on these words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bPo9Z4ohbMS"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGyYEutcoF-E"
      },
      "source": [
        "word_embedding = vanila_rnn.embeddings\n",
        "vocab = vocabulary\n",
        "print(word_embedding)\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3IQZZ27s4CC"
      },
      "source": [
        "word_embedding = (word_embedding.cpu().weight).detach().numpy()\n",
        "word_embedding"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}